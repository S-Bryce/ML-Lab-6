{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86c4bda",
   "metadata": {},
   "source": [
    "# Python Group\n",
    "## Lab Assignment Six: Convolutional Network Architectures\n",
    "### Wali Chaudhary, Bryce Shurts, & Alex Wright"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af775570",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "\n",
    "Our dataset is composed of MRI images of brains with labels indicating whether the brain has a cancerous tumor or is normal. All images are black and white, but are composed of different image types, formats, and resolutions. Each image is labeled as cancerous or normal. The MRI scan may be top down or from a side profile perspective. The purpose of this dataset is to help automate the identification of cancerous tumors from a vast quantity of data generated by the MRI machine which generates an image for each \"slice\" of the three dimensional object placed inside through use of magnetic resonance. This data is important as the process of identifying a cancerous tumor is a vast quantity of MRI data can be tedious and error prone. Automating the discovery of a tumor can help a doctor zero in on specific portions of the data for review, may detect tumors a doctor may have missed, and generally assist with the procedure of analyzing MRI data. Since medical data will be reviewed by a doctor regardless of the outcome of the classifier, less than perfect results may still be helpful in diagnosis.\n",
    "\n",
    "### Citation & Acknowledgement\n",
    "The Brain Tumor Data Set dataset is licensed under the [GNU General Public License, version 2](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html) and was provided by Preet Viradiya on [Kaggle](https://www.kaggle.com/datasets/preetviradiya/brian-tumor-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c3b6fe0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.11.0\n",
      "Keras version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "# Handle all imports for notebook\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from category_encoders import BinaryEncoder\n",
    "from category_encoders import OrdinalEncoder\n",
    "from category_encoders import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics as mt\n",
    "import pprint\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import concatenate\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import image\n",
    "import random\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from numpy import ndarray\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import random\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81b5f4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "      <th>format</th>\n",
       "      <th>mode</th>\n",
       "      <th>shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Cancer (1).jpg</td>\n",
       "      <td>tumor</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>RGB</td>\n",
       "      <td>(512, 512, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Cancer (1).png</td>\n",
       "      <td>tumor</td>\n",
       "      <td>PNG</td>\n",
       "      <td>L</td>\n",
       "      <td>(300, 240)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Cancer (1).tif</td>\n",
       "      <td>tumor</td>\n",
       "      <td>TIFF</td>\n",
       "      <td>RGB</td>\n",
       "      <td>(256, 256, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Cancer (10).jpg</td>\n",
       "      <td>tumor</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>RGB</td>\n",
       "      <td>(512, 512, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Cancer (10).tif</td>\n",
       "      <td>tumor</td>\n",
       "      <td>TIFF</td>\n",
       "      <td>RGB</td>\n",
       "      <td>(256, 256, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            image  class format mode          shape\n",
       "0           0   Cancer (1).jpg  tumor   JPEG  RGB  (512, 512, 3)\n",
       "1           1   Cancer (1).png  tumor    PNG    L     (300, 240)\n",
       "2           2   Cancer (1).tif  tumor   TIFF  RGB  (256, 256, 3)\n",
       "3           3  Cancer (10).jpg  tumor   JPEG  RGB  (512, 512, 3)\n",
       "4           4  Cancer (10).tif  tumor   TIFF  RGB  (256, 256, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df: DataFrame = pd.read_csv(\"metadata.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ecb5b",
   "metadata": {},
   "source": [
    "Immediately we can see that we have an unnamed title with a set of IDs, which can be safely removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71bdea66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "      <th>format</th>\n",
       "      <th>mode</th>\n",
       "      <th>shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cancer (1).jpg</td>\n",
       "      <td>tumor</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>RGB</td>\n",
       "      <td>(512, 512, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cancer (1).png</td>\n",
       "      <td>tumor</td>\n",
       "      <td>PNG</td>\n",
       "      <td>L</td>\n",
       "      <td>(300, 240)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cancer (1).tif</td>\n",
       "      <td>tumor</td>\n",
       "      <td>TIFF</td>\n",
       "      <td>RGB</td>\n",
       "      <td>(256, 256, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cancer (10).jpg</td>\n",
       "      <td>tumor</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>RGB</td>\n",
       "      <td>(512, 512, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cancer (10).tif</td>\n",
       "      <td>tumor</td>\n",
       "      <td>TIFF</td>\n",
       "      <td>RGB</td>\n",
       "      <td>(256, 256, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image  class format mode          shape\n",
       "0   Cancer (1).jpg  tumor   JPEG  RGB  (512, 512, 3)\n",
       "1   Cancer (1).png  tumor    PNG    L     (300, 240)\n",
       "2   Cancer (1).tif  tumor   TIFF  RGB  (256, 256, 3)\n",
       "3  Cancer (10).jpg  tumor   JPEG  RGB  (512, 512, 3)\n",
       "4  Cancer (10).tif  tumor   TIFF  RGB  (256, 256, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf2b73",
   "metadata": {},
   "source": [
    "#### Resolutions by Count\n",
    "It looks like the images are not particularly uniform in composition. Let's investigate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcfb09ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolutions by count:\n",
      " (512, 512, 3)    884\n",
      "(225, 225, 3)    364\n",
      "(630, 630, 3)    126\n",
      "(256, 256, 3)    105\n",
      "(236, 236, 3)     89\n",
      "(251, 201, 3)     80\n",
      "(217, 232, 3)     53\n",
      "(168, 300, 3)     52\n",
      "(221, 228, 3)     51\n",
      "(198, 150, 3)     45\n",
      "(252, 200, 3)     44\n",
      "(417, 428, 3)     43\n",
      "(222, 227, 3)     40\n",
      "(201, 173, 3)     38\n",
      "(244, 206, 3)     38\n",
      "(442, 442, 3)     34\n",
      "(192, 192, 3)     32\n",
      "(250, 201, 3)     31\n",
      "(231, 218, 3)     31\n",
      "(234, 215, 3)     28\n",
      "Name: shape, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "sns.countplot(x=\"format\", data=df, ax=ax[0])\n",
    "sns.countplot(x=\"mode\", data=df, ax=ax[1])\n",
    "print(\"Resolutions by count:\\n\", df[\"shape\"].value_counts()[0:20])\n",
    "# Way too many resolutions to plot...\n",
    "#sns.countplot(x=\"shape\", data=df, ax=ax[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e094a6",
   "metadata": {},
   "source": [
    "We can see that we have a few different images that are not in the JPEG format and/or are not using the RGB or L (grayscale) color mode, which are both the most common. Since conversions would be computationally expensive and lossy, and since the number of images that are in the wrong format is relatively few, they will be dropped.\n",
    "Resolutions will also be restricted to the largest square images in the top 20 most frequent resolutions, with all images to be downscaled to the smallest image to avoid losing data in trying to upscale (shown to be worse than downscaling by [this article](https://medium.com/neuronio/how-to-deal-with-image-resizing-in-deep-learning-e5177fad7d89)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a160c910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      image   class format mode          shape\n",
      "0            Cancer (1).jpg   tumor   JPEG  RGB  (512, 512, 3)\n",
      "1            Cancer (1).png   tumor    PNG    L     (300, 240)\n",
      "2            Cancer (1).tif   tumor   TIFF  RGB  (256, 256, 3)\n",
      "3           Cancer (10).jpg   tumor   JPEG  RGB  (512, 512, 3)\n",
      "4           Cancer (10).tif   tumor   TIFF  RGB  (256, 256, 3)\n",
      "...                     ...     ...    ...  ...            ...\n",
      "4595  Not Cancer  (995).jpg  normal   JPEG  RGB  (168, 300, 3)\n",
      "4596  Not Cancer  (996).jpg  normal   JPEG  RGB  (509, 452, 3)\n",
      "4597  Not Cancer  (997).jpg  normal   JPEG  RGB  (197, 177, 3)\n",
      "4598  Not Cancer  (998).jpg  normal   JPEG  RGB  (217, 232, 3)\n",
      "4599  Not Cancer  (999).jpg  normal   JPEG  RGB  (221, 228, 3)\n",
      "\n",
      "[4600 rows x 5 columns]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot perform 'ror_' with a dtyped [object] array and scalar of type [bool]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:305\u001b[0m, in \u001b[0;36mna_logical_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;66;03m# For exposition, write:\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m#  yarr = isinstance(y, np.ndarray)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;66;03m# Then Cases where this goes through without raising include:\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;66;03m#  (xint or xbool) and (yint or bool)\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/roperator.py:58\u001b[0m, in \u001b[0;36mror_\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mror_\u001b[39m(left, right):\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mor_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:319\u001b[0m, in \u001b[0;36mna_logical_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_binop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;167;01mTypeError\u001b[39;00m,\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;167;01mValueError\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;167;01mNotImplementedError\u001b[39;00m,\n\u001b[1;32m    326\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/_libs/ops.pyx:210\u001b[0m, in \u001b[0;36mpandas._libs.ops.scalar_binop\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/roperator.py:58\u001b[0m, in \u001b[0;36mror_\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mror_\u001b[39m(left, right):\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mor_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'bool' and 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m a \u001b[38;5;241m=\u001b[39m df[(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m|\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n\u001b[0;32m----> 6\u001b[0m bad_columns \u001b[38;5;241m=\u001b[39m df[(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJPEG\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m|\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m|\u001b[39m\n\u001b[1;32m      7\u001b[0m                  ((df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(630, 630, 3)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m                   \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(512, 512, 3)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m                   \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(442, 442, 3)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m                   \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(256, 256, 3)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m                   \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(236, 236, 3)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m                   \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(225, 225, 3)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m                   \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(192, 192, 3)\u001b[39m\u001b[38;5;124m\"\u001b[39m))]\n\u001b[1;32m     14\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(bad_columns\u001b[38;5;241m.\u001b[39mindex, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m bad_columns \u001b[38;5;241m=\u001b[39m bad_columns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# For later deletion\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/arraylike.py:84\u001b[0m, in \u001b[0;36mOpsMixin.__ror__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__ror__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__ror__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logical_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mror_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/series.py:6254\u001b[0m, in \u001b[0;36mSeries._logical_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6251\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   6252\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 6254\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogical_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:395\u001b[0m, in \u001b[0;36mlogical_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# For int vs int `^`, `|`, `&` are bitwise operators and return\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m#   integer dtypes.  Otherwise these are boolean ops\u001b[39;00m\n\u001b[1;32m    393\u001b[0m filler \u001b[38;5;241m=\u001b[39m fill_int \u001b[38;5;28;01mif\u001b[39;00m is_self_int_dtype \u001b[38;5;129;01mand\u001b[39;00m is_other_int_dtype \u001b[38;5;28;01melse\u001b[39;00m fill_bool\n\u001b[0;32m--> 395\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mna_logical_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# error: Cannot call function of unknown type\u001b[39;00m\n\u001b[1;32m    397\u001b[0m res_values \u001b[38;5;241m=\u001b[39m filler(res_values)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:328\u001b[0m, in \u001b[0;36mna_logical_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;167;01mTypeError\u001b[39;00m,\n\u001b[1;32m    322\u001b[0m             \u001b[38;5;167;01mValueError\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[38;5;167;01mNotImplementedError\u001b[39;00m,\n\u001b[1;32m    326\u001b[0m         ) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    327\u001b[0m             typ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(y)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 328\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    329\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot perform \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with a dtyped [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] array \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    330\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand scalar of type [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtyp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot perform 'ror_' with a dtyped [object] array and scalar of type [bool]"
     ]
    }
   ],
   "source": [
    "# First we identify undesirable images\n",
    "# TODO: convert RGBA to RGB?\n",
    "\n",
    "a = df[(df[\"mode\"] != \"RGB\") | (df[\"mode\"] != \"L\")]\n",
    "print(a)\n",
    "bad_columns = df[(df[\"format\"] != \"JPEG\") | (df[\"mode\"] != \"RGB\" | df[\"mode\"] != \"L\") |\n",
    "                 ((df[\"shape\"] != \"(630, 630, 3)\")\n",
    "                  & (df[\"shape\"] != \"(512, 512, 3)\")\n",
    "                  & (df[\"shape\"] != \"(442, 442, 3)\")\n",
    "                  & (df[\"shape\"] != \"(256, 256, 3)\")\n",
    "                  & (df[\"shape\"] != \"(236, 236, 3)\")\n",
    "                  & (df[\"shape\"] != \"(225, 225, 3)\")\n",
    "                  & (df[\"shape\"] != \"(192, 192, 3)\"))]\n",
    "df.drop(bad_columns.index, inplace=True)\n",
    "bad_columns = bad_columns[\"image\"] # For later deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f08cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we remove images we don't want to convert or otherwise process\n",
    "# We could import the images first and then delete from the numpy arrays to preserve the github dataset, but that's a lot of wasted time & space.\n",
    "files = [file for file in listdir(\"Brain Tumor Data Set/Brain Tumor/\") if isfile(join(\"Brain Tumor Data Set/Brain Tumor/\", file))] + [file for file in listdir(\"Brain Tumor Data Set/Healthy/\") if isfile(join(\"Brain Tumor Data Set/Healthy/\", file))]\n",
    "bad_files = [file for file in files if file in bad_columns.tolist()]\n",
    "for file in bad_files:\n",
    "    if \"Not\" in file:\n",
    "        os.remove(join(\"Brain Tumor Data Set/Healthy/\", file))\n",
    "    else:\n",
    "        os.remove(join(\"Brain Tumor Data Set/Brain Tumor/\", file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8388c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm we still have more than 1000 images\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now import each image as a numpy array & do the final conversion of their resolutions\n",
    "# Thoughts on what to do with resolutions? I say convert all to the smallest dimension (225x225)\n",
    "cancer_images = []\n",
    "healthy_images = []\n",
    "original_images = [file for file in listdir(\"Brain Tumor Data Set/Brain Tumor/\") if isfile(join(\"Brain Tumor Data Set/Brain Tumor/\", file))] + [file for file in listdir(\"Brain Tumor Data Set/Healthy/\") if isfile(join(\"Brain Tumor Data Set/Healthy/\", file))]\n",
    "for image in original_images:\n",
    "    if \"Not\" in image:\n",
    "        cancer_images.append(plt.imread(join(\"Brain Tumor Data Set/Healthy/\", image)))\n",
    "    else:\n",
    "        healthy_images.append(plt.imread(join(\"Brain Tumor Data Set/Brain Tumor/\", image)))\n",
    "\n",
    "images = []\n",
    "for image_set in (cancer_images, healthy_images): \n",
    "    for image in image_set:\n",
    "        # Apply the resizing non-uniformly since evidently it affects images\n",
    "        # even if they're the same size as the target resolution.\n",
    "        # Difference with Linear PCA was:\n",
    "        #   Applied to every image:\n",
    "        #     233 components with λ > 1 for 92% explanation\n",
    "        #   Applied to mis-sized images:\n",
    "        #     256 components with λ > 1 for 99% explanation\n",
    "        \n",
    "        #images.append(resize(image, (192, 192), anti_aliasing=True).flatten())\n",
    "        if len(image) != 192:\n",
    "            resized = resize(image, (192, 192), anti_aliasing=True)\n",
    "            #print(resized.shape)\n",
    "            images.append(resized)\n",
    "        else:\n",
    "            #print(image.shape)\n",
    "            images.append(image)\n",
    "images = np.array(images)\n",
    "\n",
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize=(10,10))\n",
    "fig.suptitle(\"Downscaling Quality Comparisons\")\n",
    "subfigs = fig.subfigures(2, 1)\n",
    "ax_cancer = subfigs[0].subplots(1, 2)\n",
    "subfigs[0].suptitle(\"Brain With Tumor\")\n",
    "ax_cancer[0].imshow(cancer_images[0], cmap='gray')\n",
    "ax_cancer[0].set_title(\"Original Image\")\n",
    "ax_cancer[1].imshow(images[0].reshape(192, 192, 3), cmap='gray')\n",
    "ax_cancer[1].set_title(\"Downscaled Image\")\n",
    "ax_healthy = subfigs[1].subplots(1, 2)\n",
    "subfigs[1].suptitle(\"Brain Without Tumor\")\n",
    "ax_healthy[0].imshow(healthy_images[0], cmap='gray')\n",
    "ax_healthy[0].set_title(\"Original Image\")\n",
    "ax_healthy[1].imshow(images[len(cancer_images)].reshape(192, 192, 3), cmap='gray')\n",
    "ax_healthy[1].set_title(\"Downscaled Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5263d747-1b0a-4177-a98c-8deda80ea924",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e08be7-0abe-4d2f-8d67-c0d242966e4f",
   "metadata": {},
   "source": [
    "#### Measurements Chosen\n",
    "\n",
    "Ultimately, we chose the F1 score as our performance metric for the models. There are several factors we had to take to chose our performance metric for this particular task for the identification of Brain Tumors vs Healthy brains.\n",
    "\n",
    "First, we had to consider the nature of the task at hand, which is classifying tumors in brains. Because this is a task with a high cost of misclassification, it is important for us to choose a performance metric which takes into account the trade-off between false positives and false negatives, and because the F1 score is the harmonic mean of precision and recall, it allows us to evaluate our model's ability to identify each class correctly and minimize both false positives and false negatives. This will ultimately help us choose the best model with the lowest false positive and false negative rates.\n",
    "\n",
    "Secondly, initially we had many numerous image resolutions (see the Resolution by Count subsection above), each with varying degrees of frequency of images. Although we resolved this issue by dropping certain image resolutions until we only had 6 and also removed any images that were not JPEG or in RGB which made the data more uniform, there is still a high chance of an imbalanced class distribution. This is a problem because the model might perform well on the majority class but perform poorly on a minority class, but still achieve high accuracy. This is also another reason to use an additional metric such as the F1 score to give us further insight into model performance.\n",
    "\n",
    "In addition to the F1-score, we will use a confusion matrix to visualize the performance of our model. This will provide insights into the specific misclassifications made by the model, helping us understand its strengths and weaknesses across different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f659c-5452-4c7b-93cf-f660e058934c",
   "metadata": {},
   "source": [
    "#### Dividing Data\n",
    "\n",
    "We will use Stratified Shuffle Split so that we can preserve class distribution, meaning that we can preserve proportionality per class in our training and testing splits. It also allows us to introduce a degree of randomization in the data splitting process, reducing the probability of overfitting and helps our model generalize better. We can also specify the proportion we want to split by, so this gives us some additional flexibility and an additional parameter for us to experiment with finding the ideal test-train data proportional split. Given the additional randomization componenet of Stratified Shuffle Split, it mirrors the real world better because our model will encounter random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a3c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_labels = df.loc[:,\"class\"]\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# le.fit(y_labels)\n",
    "# y = le.transform(y_labels)\n",
    "\n",
    "# X = images\n",
    "\n",
    "\n",
    "# # Split it into train / test subsets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394b1821-d158-4ed6-a725-47296b0f7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Assuming 'df' is your DataFrame containing the class labels and 'images' is a NumPy array of image data\n",
    "y_labels = df.loc[:, \"class\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_labels)\n",
    "y = le.transform(y_labels)\n",
    "\n",
    "X = images\n",
    "\n",
    "# Define the Stratified Shuffle Split object\n",
    "n_splits = 5\n",
    "test_size = 0.2\n",
    "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=42)\n",
    "\n",
    "# Perform Stratified Shuffle Split cross-validation\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    # Train and evaluate your model on the current split here\n",
    "    # ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb2381",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95737a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# since this is not binary, we should go ahead and one-hot encode the inputs\n",
    "y_train_ohe = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_ohe = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# make a 3 layer keras MLP\n",
    "mlp = Sequential()\n",
    "mlp.add( Flatten() ) # make images flat for the MLP input\n",
    "mlp.add( Dense(input_dim=1, units=30, \n",
    "               activation='relu') )\n",
    "mlp.add( Dense(units=15, activation='relu') )\n",
    "mlp.add( Dense(NUM_CLASSES) )\n",
    "mlp.add( Activation('softmax') )\n",
    "\n",
    "mlp.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mlp.fit(X_train, y_train_ohe, \n",
    "        batch_size=32, epochs=150, \n",
    "        shuffle=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e69c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# make a CNN with conv layer and max pooling\n",
    "cnn = Sequential() \n",
    "cnn.add( Conv2D(filters=16, kernel_size= (2, 2), padding='same', \n",
    "                input_shape=(192,192,3),\n",
    "               ) )\n",
    "\n",
    "cnn.add( MaxPooling2D(pool_size=(2, 2)) )\n",
    "cnn.add( Activation('relu') )\n",
    "# add one layer on flattened output\n",
    "cnn.add( Flatten() )\n",
    "cnn.add( Dense(NUM_CLASSES) )\n",
    "cnn.add( Activation('softmax') )\n",
    "\n",
    "cnn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a77a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "print(X_train.shape)\n",
    "# Let's train the model \n",
    "cnn.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn.fit(X_train, y_train_ohe, \n",
    "        batch_size=32, epochs=150, \n",
    "        shuffle=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def compare_mlp_cnn(cnn, mlp, X_test, y_test, labels='auto'):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    if cnn is not None:\n",
    "        yhat_cnn = np.argmax(cnn.predict(X_test), axis=1)\n",
    "        acc_cnn = mt.accuracy_score(y_test,yhat_cnn)\n",
    "        plt.subplot(1,2,1)\n",
    "        cm = mt.confusion_matrix(y_test,yhat_cnn)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm, annot=True, fmt='.2f',xticklabels=labels,yticklabels=labels)\n",
    "        plt.title('CNN: '+str(acc_cnn))\n",
    "    \n",
    "    if mlp is not None:\n",
    "        yhat_mlp = np.argmax(mlp.predict(X_test), axis=1)\n",
    "        acc_mlp = mt.accuracy_score(y_test,yhat_mlp)\n",
    "        plt.subplot(1,2,2)\n",
    "        cm = mt.confusion_matrix(y_test,yhat_mlp)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm,annot=True, fmt='.2f',xticklabels=labels,yticklabels=labels)\n",
    "        plt.title('MLP: '+str(acc_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0811db",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(cnn,mlp,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c21b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# changes: \n",
    "#    1. increased kernel size\n",
    "cnn2 = Sequential()\n",
    "cnn2.add( Conv2D(filters=16, kernel_size= (3, 3), \n",
    "                padding='same', input_shape=(192,192,3),\n",
    "                ) )\n",
    "cnn2.add( Activation('relu') )\n",
    "cnn2.add( MaxPooling2D(pool_size=(2, 2)) )\n",
    "# add one layer on flattened output\n",
    "cnn2.add( Flatten() )\n",
    "cnn2.add( Dense(NUM_CLASSES, activation='softmax') )\n",
    "\n",
    "# Let's train the model \n",
    "cnn2.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn2.fit(X_train, y_train_ohe, \n",
    "        batch_size=32, epochs=150, \n",
    "        shuffle=True, verbose=0)\n",
    "\n",
    "compare_mlp_cnn(cnn2,mlp,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a691faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# changes: \n",
    "#    1. increased kernel size\n",
    "#    2. add another conv/pool layer \n",
    "cnn3 = Sequential()\n",
    "\n",
    "num_filt_layers = [32, 32]\n",
    "for num_filters in num_filt_layers:\n",
    "    cnn3.add( Conv2D(filters=num_filters, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding='same',\n",
    "                     ) )\n",
    "    cnn3.add( Activation('relu'))\n",
    "    cnn3.add( MaxPooling2D(pool_size=(2, 2)) )\n",
    "    \n",
    "\n",
    "# add one layer on flattened output\n",
    "cnn3.add( Flatten() )\n",
    "cnn3.add( Dense(NUM_CLASSES) )\n",
    "cnn3.add( Activation('softmax') )\n",
    "\n",
    "# Let's train the model \n",
    "cnn3.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn3.fit(X_train, y_train_ohe, \n",
    "        batch_size=32, epochs=150, \n",
    "        shuffle=True, verbose=0)\n",
    "\n",
    "compare_mlp_cnn(cnn3,mlp,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7395f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# changes: \n",
    "#    1. increased kernel size\n",
    "#    2. add another conv/pool layer with increasing num filters\n",
    "#    3. add more layers once flattened\n",
    "cnn4 = Sequential()\n",
    "\n",
    "num_filt_layers = [16, 32]\n",
    "for num_filters in num_filt_layers:\n",
    "    cnn4.add( Conv2D(filters=num_filters, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding='same'))\n",
    "    cnn4.add( Activation('relu'))\n",
    "    cnn4.add( MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "\n",
    "# add one layer on flattened output\n",
    "cnn4.add( Flatten() )\n",
    "cnn4.add( Dense(100) )\n",
    "cnn4.add( Activation('relu') )\n",
    "cnn4.add( Dense(NUM_CLASSES) )\n",
    "cnn4.add( Activation('softmax') )\n",
    "\n",
    "# Let's train the model \n",
    "cnn4.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn4.fit(X_train, y_train_ohe, \n",
    "        batch_size=32, epochs=150, \n",
    "        shuffle=True, verbose=0)\n",
    "\n",
    "compare_mlp_cnn(cnn4,mlp,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a53d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school-ipython-kernel",
   "language": "python",
   "name": "school-ipython-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
